<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>A random walk through HMM (2) - structure and inference - Taotao Tan</title><meta name="Description" content="Taotao&#39;s Blog Posts"><meta property="og:title" content="A random walk through HMM (2) - structure and inference" />
<meta property="og:description" content="Introduction I found most of HMM tutorial are presented in a top-down manner, where a lot of mathematical notations are thrown to the learners without a concrete example. This could sometimes make the learning process very frustrating, at least for me. In this post, I will try to demonstrate the core idea of HMM, as well as a commonly-used inference algorithm using a toy example. The example is from Dr.Xiaole Liu&rsquo;s Youtube channel, and I highly recommend you to check out her video if you want to develop intuition of HMM rather than get killed by notations." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jasontan-code.github.io/posts/hmm2/" />
<meta property="og:image" content="https://jasontan-code.github.io/images/avatar.png"/>
<meta property="article:published_time" content="2021-12-04T15:19:26-06:00" />
<meta property="article:modified_time" content="2021-12-04T15:19:26-06:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://jasontan-code.github.io/images/avatar.png"/>

<meta name="twitter:title" content="A random walk through HMM (2) - structure and inference"/>
<meta name="twitter:description" content="Introduction I found most of HMM tutorial are presented in a top-down manner, where a lot of mathematical notations are thrown to the learners without a concrete example. This could sometimes make the learning process very frustrating, at least for me. In this post, I will try to demonstrate the core idea of HMM, as well as a commonly-used inference algorithm using a toy example. The example is from Dr.Xiaole Liu&rsquo;s Youtube channel, and I highly recommend you to check out her video if you want to develop intuition of HMM rather than get killed by notations."/>
<meta name="application-name" content="Taotao&#39;s Blog Posts">
<meta name="apple-mobile-web-app-title" content="Taotao&#39;s Blog Posts"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://jasontan-code.github.io/posts/hmm2/" /><link rel="prev" href="https://jasontan-code.github.io/posts/hmm1/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "A random walk through HMM (2) - structure and inference",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/jasontan-code.github.io\/posts\/hmm2\/"
        },"genre": "posts","keywords": "Probability","wordcount":  2628 ,
        "url": "https:\/\/jasontan-code.github.io\/posts\/hmm2\/","datePublished": "2021-12-04T15:19:26-06:00","dateModified": "2021-12-04T15:19:26-06:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "Taotao"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Taotao Tan">Taotao Tan</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/about/"> About </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Taotao Tan">Taotao Tan</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/about/" title="">About</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">A random walk through HMM (2) - structure and inference</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>Taotao</a></span>&nbsp;<span class="post-category">included in <a href="/categories/probability/"><i class="far fa-folder fa-fw"></i>Probability</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-12-04">2021-12-04</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;2628 words&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;13 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="true">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#set-up-the-model">Set up the model</a></li>
    <li><a href="#brutal-force-inference">Brutal force inference</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#forward-backward-algorithm-inference">Forward Backward algorithm inference</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="introduction">Introduction</h2>
<p>I found most of HMM tutorial are presented in a top-down manner, where a lot of mathematical notations are thrown to the learners without a concrete example. This could sometimes make the learning process very frustrating, at least for me. In this post, I will try to demonstrate the core idea of HMM, as well as a commonly-used inference algorithm using a toy example. The example is from <a href="https://www.youtube.com/watch?v=VBs8FYsZIN4" target="_blank" rel="noopener noreffer">Dr.Xiaole Liu&rsquo;s Youtube channel</a>, and I highly recommend you to check out her video if you want to develop intuition of HMM rather than get killed by notations. Also, you may want to review <a href="https://en.wikipedia.org/wiki/Conditional_independence" target="_blank" rel="noopener noreffer">conditional independence</a> before you start reading, since a lot of derivations will use this property.</p>
<p> <br>
 <br>
 </p>
<h2 id="set-up-the-model">Set up the model</h2>
<p>I have a biased coin and a fair coin. Each time I toss a coin under the table, and will show you the outcome (Head or Tail). After I toss this coin, There will be a chance that I will switch to a different coin or to continue to use the same coin, and make another toss under the table. Again, <strong>what you can see is the outcome of the coin toss, but you don&rsquo;t know if I used a fair coin or a biased coin</strong>. <strong>Your job is to make an educated guess of which coin I have used to generate the sequence of outcomes</strong>. To make the example more concrete, imagine I tossed the coin three times, and the outcome is Head - Tail - Tail, as the diagram shows:</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/HMM2/HMM1.png"
        data-srcset="/images/HMM2/HMM1.png, /images/HMM2/HMM1.png 1.5x, /images/HMM2/HMM1.png 2x"
        data-sizes="auto"
        alt="/images/HMM2/HMM1.png"
        title="/images/HMM2/HMM1.png" /></p>
<p> </p>
<p>Besides the sequence of the outcome, you have another three pieces of information to help you make this educated guess:</p>
<ol>
<li>Initial probability of which coin I used.</li>
</ol>
<p> </p>
<ol start="2">
<li>Transition probability, meaning the probability that I switch from a Fair coin to a Biased coin, or vice versa. In the following graph, we are saying if I use a fair coin this time, then I have have 90% of chance to continue to use a fair coin, and 10% of chance to switch to a biased coin. If I used a biased coin this time, I will have 30% of chance to switch to a fair coin next time, and 70% of chance to use the same coin.</li>
</ol>
<p> </p>
<ol start="3">
<li>Emission probability, meaning the probability of getting Head/Tail, given the coin is Fair/Biased. Here, we will have 50% of chance to get Head/Tail if I am using a fair coin. However, if I am using a biased coin, I will have 80% chance to get a head, and 20% chance to get a tail.</li>
</ol>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/HMM2/HMM2.png"
        data-srcset="/images/HMM2/HMM2.png, /images/HMM2/HMM2.png 1.5x, /images/HMM2/HMM2.png 2x"
        data-sizes="auto"
        alt="/images/HMM2/HMM2.png"
        title="/images/HMM2/HMM2.png" /></p>
<p> </p>
<p>I think it&rsquo;s important to first clarify some notations. Here is a few examples to help demonstrate:</p>
<p> </p>
<p>The probability of getting a fair coin at the beginning (time 1) can be denoted as $P(F_1) = 0.4$</p>
<p>The probability of switching to a fair coin at time 3, given that we see a biased coin at time 2 can be denoted as $P(F_3 | B_2) = 0.3$</p>
<p>The probability of seeing a tail at time 2, given that we have a fair coin at time 2 can be denoted as $P(T_2 | F_2) = 0.5$</p>
<p> <br>
 <br>
 </p>
<h2 id="brutal-force-inference">Brutal force inference</h2>
<p>Before we jump into the detail, let me reiterate the goal: <strong>we want to guess the most possible sequence of states that generates the outcome. In our example, when did I use a fair coin and when did I use a biased coin.</strong></p>
<p> </p>
<p>Let&rsquo;s first assume our coin sequence is <strong>Biased &ndash; Biased &ndash; Fair</strong>. The question is: what is the probability of seeing this coin sequence and the outcome sequence.</p>
<p>What we need to do here is simply multiply all the transition probability (including the <strong>initial probability of getting a biased coin</strong>) from <strong>Biased &ndash; Biased &ndash; Fair</strong> and their emission probability <strong>Biased &ndash; Head</strong>, <strong>Biased &ndash; Tail</strong>,  <strong>Fair &ndash; Head</strong>.</p>
<p>$$
\begin{aligned}
P(B_1, B_2, F_3, H_1, T_2, T_3)  &amp;= P(B_1) \cdot P(H_1 | B_1) \cdot P(B_2 | B_1)\cdot P(T_2 | B_2) \cdot P(F_3 | B_2)\cdot P(T_3 | F_3) \\
&amp;= 0.6 \times 0.8 \times 0.7 \times 0.2 \times 0.3 \times 0.5 \\
&amp; = 0.01008
\end{aligned}
$$</p>
<p> </p>
<p>Of course there are more than one way to get a sequence of <strong>Head &ndash; Tail &ndash; Tail</strong>. For example, to get <strong>Head &ndash; Tail &ndash; Tail</strong>, we can have our coin sequence to be <strong>Fair &ndash; Fair &ndash; Biased</strong>, <strong>Biased &ndash; Fair &ndash; Biased</strong>, etc. In fact there are $2^3= 8$ possible ways to get <strong>Head &ndash; Tail &ndash; Tail</strong>, and here is the table with their probabilities.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Coin sequence</th>
<th style="text-align:right">Probability of coin sequence <strong>and</strong> outcome sequence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$F_1, F_2, F_3$</td>
<td style="text-align:right">$P(F_1, F_2, F_3, H_1, T_2, T_3) = 0.0405$</td>
</tr>
<tr>
<td style="text-align:left">$F_1, F_2, B_3$</td>
<td style="text-align:right">$P(F_1, F_2, B_3, H_1, T_2, T_3) = 0.0018$</td>
</tr>
<tr>
<td style="text-align:left">$F_1, B_2, F_3$</td>
<td style="text-align:right">$P(F_1, B_2, F_3, H_1, T_2, T_3) = 0.0006$</td>
</tr>
<tr>
<td style="text-align:left">$F_1, B_2, B_3$</td>
<td style="text-align:right">$P(F_1, B_2, B_3, H_1, T_2, T_3) = 0.00056$</td>
</tr>
<tr>
<td style="text-align:left">$B_1, F_2, F_3$</td>
<td style="text-align:right">$P(B_1, F_2, F_3, H_1, T_2, T_3) = 0.0324$</td>
</tr>
<tr>
<td style="text-align:left">$B_1, F_2, B_3$</td>
<td style="text-align:right">$P(B_1, F_2, B_3, H_1, T_2, T_3) = 0.00144$</td>
</tr>
<tr>
<td style="text-align:left">$B_1, B_2, F_3$</td>
<td style="text-align:right">$P(B_1, B_2, F_3, H_1, T_2, T_3) = 0.01008$</td>
</tr>
<tr>
<td style="text-align:left">$B_1, B_2, B_3$</td>
<td style="text-align:right">$P(B_1, B_2, B_3, H_1, T_2, T_3) = 0.009408$</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Here we have enumerated all possible states. If we sum up all the values in the above table, we can achieve the probability of observing this sequence of outcome:
$$
\begin{aligned}
P(H_1, T_2, T_3) &amp;= P(F_1, F_2, F_3, H_1, T_2, T_3) + P(F_1, F_2, B_3, H_1, T_2, T_3) + &hellip; + P(B_1, B_2, B_3, H_1, T_2, T_3) \\
&amp; = 0.0405 + 0.0018 + 0.0006 + 0.00056 + 0.0324 + 0.00144 + 0.01008 + 0.009408 \\
&amp; = 0.096788
\end{aligned}
$$</p>
<p> </p>
<p>Using conditional probability, we can achieve $P(\text{coin sequence} |H_1, T_2, T_3)$. For example, we can calculate the probability of our outcome sequence is from <strong>Fair &ndash; Fair &ndash; Fair</strong>:</p>
<p>$$
\begin{aligned}
P(F_1, F_2, F_3 |H_1, T_2, T_3) &amp; = \frac{P(F_1, F_2, F_3 ,H_1, T_2, T_3) }{P(H_1, T_2, T_3)} \\
&amp; = \frac{0.0405}{0.096788} \\
&amp; = 0.418
\end{aligned}
$$</p>
<p>That means there are 41.8% of the chance that the outcome sequence is from <strong>Fair &ndash; Fair &ndash; Fair</strong>.</p>
<p> </p>
<p>To see the whole picture, let&rsquo;s calculate the probability of every possible coin sequence, given we observed <strong>Head &ndash; Tail &ndash;Tail</strong> outcome sequence:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Coin sequence</th>
<th style="text-align:right">Probability of coin sequence <strong>given</strong> outcome sequence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$F_1, F_2, F_3$</td>
<td style="text-align:right">$0.0405/0.096788= 0.4184403 $</td>
</tr>
<tr>
<td style="text-align:left">$F_1, F_2, B_3$</td>
<td style="text-align:right">$0.0018/0.096788= 0.01859735 $</td>
</tr>
<tr>
<td style="text-align:left">$F_1, B_2, F_3$</td>
<td style="text-align:right">$0.0006/0.096788= 0.006199116 $</td>
</tr>
<tr>
<td style="text-align:left">$F_1, B_2, B_3$</td>
<td style="text-align:right">$0.00056/0.096788= 0.005785841 $</td>
</tr>
<tr>
<td style="text-align:left">$B_1, F_2, F_3$</td>
<td style="text-align:right">$0.0324/0.096788= 0.3347522 $</td>
</tr>
<tr>
<td style="text-align:left">$B_1, F_2, B_3$</td>
<td style="text-align:right">$0.00144/0.096788= 0.01487788 $</td>
</tr>
<tr>
<td style="text-align:left">$B_1, B_2, F_3$</td>
<td style="text-align:right">$0.01008/0.096788= 0.1041451 $</td>
</tr>
<tr>
<td style="text-align:left">$B_1, B_2, B_3$</td>
<td style="text-align:right">$0.009408/0.096788= 0.09720213 $</td>
</tr>
</tbody>
</table>
<p> <br>
 </p>
<p>According to this table, the most likely coin sequence is <strong>Fair &ndash; Fair &ndash; Fair</strong>, which has 41.8% of the chance. So here is our final answer:</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/HMM2/HMM3.png"
        data-srcset="/images/HMM2/HMM3.png, /images/HMM2/HMM3.png 1.5x, /images/HMM2/HMM3.png 2x"
        data-sizes="auto"
        alt="/images/HMM2/HMM3.png"
        title="/images/HMM2/HMM3.png" /></p>
<p> </p>
<p>Besides inferring the coin sequence, we can also calculate the probability of head/tail at a given time. For example, we can calculate $P(F_2 |H_1, T_2, T_3 )$ by summing up all cases when the second coin is Fair, divided by all outcomes:</p>
<p>$$
\begin{aligned}
P(F_2 |H_1, T_2, T_3 ) &amp; = \frac{P(F_2 ,H_1, T_2, T_3) }{P(H_1, T_2, T_3)} \\
&amp; = \frac{P(F_1, F_2, F_3,H_1, T_2, T_3) + P(F_1, F_2, B_3,H_1, T_2, T_3) + P(B_1, F_2, F_3,H_1, T_2, T_3)  + P(B_1, F_2, B_3,H_1, T_2, T_3)  }{P(H_1, T_2, T_3)} \\
&amp; = \frac{0.0405 + 0.0018 + 0.0324 + 0.00144}{0.096788} \\
&amp; = 0.7867
\end{aligned}
$$</p>
<p>The results are telling us that we are 78% sure that the second toss is from a fair coin.</p>
<p> </p>
<h4 id="a-drawback">A drawback</h4>
<p>The brutal force approach can help us develop intuition, but it should never be used to solve real-world problem. The biggest issue is the computation complexity. In our toy example, we have 3 observations, and we need to enumerate all $2^3 = 8$ cases in order to infer the hidden state. Imagine if we have 100 observations, we will have to enumerate all $2^{100}$ cases, which is never practical.</p>
<p> <br>
 <br>
 <br>
 </p>
<h2 id="forward-backward-algorithm-inference">Forward Backward algorithm inference</h2>
<p>I truly believe the guy who developed this method must be a genius. The beauty of this method is that we can infer the hidden states with a much lower computational complexity, and we can find the probability of Head/Tail at a given time point. The essence of this method is called <strong>dynamic programming</strong>, and I will demonstrate the idea below:</p>
<p> </p>
<h4 id="forward-algorithm">Forward algorithm</h4>
<p>We first need to define a few quantities $\alpha_i(F)$ and $\alpha_i(B)$. It is the joint probability of all <strong>previously observed outcomes</strong> and the <strong>current state</strong>.</p>
<p> </p>
<p>What do I mean by that? For example. $\alpha_2(F) = P(H_1, T_2, F_2)$. Similarly $\alpha_2(B) = P(H_1, T_2, B_2)$, $\alpha_3(B) = P(H_1, T_2,T_3, B_3)$, etc &hellip; (I hope the pattern is clear)</p>
<p> </p>
<p>The quantities themselves might not be very interpretable, but a nice feature of this quantity is that each $\alpha$ can be derived from the previous $\alpha$. Let&rsquo;s calculate them step by step:</p>
<blockquote>
<h4 id="calculate-alpha_1f-and-alpha_1b">Calculate $\alpha_1(F)$ and $\alpha_1(B)$:</h4>
<p>$\alpha_1(F) = P(H_1, F_1) = P(F_1) \cdot P(H_1 |F_1) = 0.4 \times 0.5 = 0.2$</p>
<p>$\alpha_1(B) = P(H_1, B_1) = P(B_1) \cdot P(H_1 |B_1) = 0.6 \times 0.8 = 0.48$</p>
</blockquote>
<p> </p>
<blockquote>
<h4 id="calculate-alpha_2f-and-alpha_2b">Calculate $\alpha_2(F)$ and $\alpha_2(B)$:</h4>
<p>$$
\begin{aligned}
\alpha_2(F) &amp;= P(H_1, T_2, F_2) \\
&amp;= [P(H_1, F_1, F_2) + P(H_1, B_1, F_2)] \cdot P(T_2 |F_2) \\
&amp; = [P(H_1, F_1) \cdot P(F_2 |F_1) + P(H_1, B_1) \cdot P(F_2 |B_1)]\cdot P(T_2 |F_2) \\
&amp; =  [ \color{red}  \alpha_1(F) \color{black}  \cdot P(F_2 |F_1) + \color{red}  \alpha_1(B) \color{black}  \cdot P(F_2 |B_1)]\cdot P(T_2 |F_2) \\
&amp; = (0.2 \times 0.9 + 0.48	 \times 0.3)\times 0.5 \\
&amp; = 0.162
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\alpha_2(B) &amp;= P(H_1, T_2, B_2) \\
&amp;= [P(H_1, F_1, B_2) + P(H_1, B_1, B_2)] \cdot P(T_2 |B_2) \\
&amp; = [P(H_1, F_1) \cdot P(B_2 |F_1) + P(H_1, B_1) \cdot P(B_2 |B_1)]\cdot P(T_2 |B_2) \\
&amp; = [ \color{red}  \alpha_1(F) \color{black}  \cdot P(B_2 |F_1) + \color{red}  \alpha_1(B) \color{black}  \cdot P(B_2 |B_1)]\cdot P(T_2 |B_2) \\
&amp; = (0.2 \times 0.1 + 0.48	 \times 0.7)\times 0.2 \\
&amp; = 0.0712
\end{aligned}
$$</p>
<p>I know the math is very messy, but here is a graphical demonstration of calculating $\alpha_2(F)$. It&rsquo;s important to understand the pattern, and the same pattern can be applied to $\alpha_3(F)$ and $\alpha_3(B)$</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/HMM2/HMM4.png"
        data-srcset="/images/HMM2/HMM4.png, /images/HMM2/HMM4.png 1.5x, /images/HMM2/HMM4.png 2x"
        data-sizes="auto"
        alt="/images/HMM2/HMM4.png"
        title="/images/HMM2/HMM4.png" /></p>
</blockquote>
<p> </p>
<blockquote>
<h4 id="calculate-alpha_3f-and-alpha_3b">Calculate $\alpha_3(F)$ and $\alpha_3(B)$:</h4>
<p>Instead of deriving the formula from scratch, I will simply use the pattern above</p>
<p>$$
\begin{aligned}
\alpha_3(F) &amp; = P(H_1, T_2,T_3, F_3) \\
&amp;= [ \color{red}  \alpha_2(F) \color{black}  \cdot P(F_3 |F_2) + \color{red}  \alpha_2(B) \color{black}  \cdot P(F_3 |B_2)]\cdot P(T_3 |F_3) \\
&amp; = (0.162 \times 0.9 + 0.0712 \times 0.3) \times 0.5 \\
&amp; = 0.08358
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\alpha_3(B) &amp; = P(H_1, T_2,T_3, B_3) \\
&amp;= [ \color{red}  \alpha_2(F) \color{black}  \cdot P(B_3 |F_2) + \color{red}  \alpha_2(B) \color{black}  \cdot P(B_3 |B_2)]\cdot P(T_3 |B_3) \\
&amp; = (0.162 \times 0.1 + 0.0712 \times 0.7) \times 0.2 \\
&amp; = 0.013208
\end{aligned}
$$</p>
</blockquote>
<p>I have color-coded the $\alpha$ in the equations. It&rsquo;s important to notice that the next $\alpha$ can be derived from the previous $\alpha$ with the same pattern. It&rsquo;s usage will be demonstrated in the next two sections.</p>
<p> <br>
 <br>
 </p>
<h4 id="backward-algorithm">Backward algorithm</h4>
<p>Similar to $\alpha_i(F)$ and $\alpha_i(B)$, we will need to define another quantity called $\beta_i(F)$ and $\beta_i(B)$, which is the conditional probability of all the observations afterwards, given that the current coin is fair/biased. For example, $\beta_2(F) = P(T_3 |F_2)$, $\beta_1(B) = P(T_2, T_3 |B_1)$. Specifically, the last $\beta_i$, which is $\beta_3(F)$ and $\beta_3(B)$ in our example, is set to $1$.</p>
<p> </p>
<p>I bet you will ask what is the point of defining this $\beta$ quantity. Please bare with me, it will be all clarified after I show you the mechanics.</p>
<p> </p>
<blockquote>
<h4 id="calculate-beta_3f-and-beta_3b">Calculate $\beta_3(F)$ and $\beta_3(B)$:</h4>
<p>As I said above, the last $\beta$ is set to $1$, so we have:</p>
<p>$\beta_3(F) = 1$</p>
<p>$\beta_3(B) = 1$</p>
</blockquote>
<p> </p>
<blockquote>
<h4 id="calculate-beta_2f-and-beta_2b">Calculate $\beta_2(F)$ and $\beta_2(B)$:</h4>
<p>$$
\begin{aligned}
\beta_2(F) &amp; = P(T_3|F_2) \\
&amp; = P(T_3, F_3 | F_2) + P(T_3, B_3 | F_2) \\
&amp; = P(F_3 | F_2) \cdot P(T_3 | F_3) + P(B_3 | F_2) \cdot P(T_3 | B_3) \\
&amp; = P(F_3 | F_2) \cdot P(T_3 | F_3) \cdot \color{red} \beta_3(F) \color{black} + P(B_3 | F_2) \cdot P(T_3 | B_3) \cdot \color{red} \beta_3(B) \color{black} \\
&amp; = 0.9 \times 0.5 \times 1 + 0.1 \times 0.2 \times 1 \\
&amp; = 0.47
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\beta_2(B) &amp; = P(T_3|B_2) \\
&amp; = P(T_3, F_3 | B_2) + P(T_3, B_3 | B_2) \\
&amp; = P(F_3 | B_2) \cdot P(T_3 | F_3) + P(B_3 | B_2) \cdot P(T_3 | B_3) \\
&amp; = P(F_3 | B_2) \cdot P(T_3 | F_3) \cdot \color{red} \beta_3(F) \color{black} + P(B_3 | B_2) \cdot P(T_3 | B_3) \cdot \color{red} \beta_3(B) \color{black} \\
&amp; = 0.3 \times 0.5 \times 1 + 0.7 \times 0.2 \times 1 \\
&amp; = 0.29
\end{aligned}
$$</p>
<p>Again, the pattern of calculating $\beta$ is important.</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/images/HMM2/HMM5.png"
        data-srcset="/images/HMM2/HMM5.png, /images/HMM2/HMM5.png 1.5x, /images/HMM2/HMM5.png 2x"
        data-sizes="auto"
        alt="/images/HMM2/HMM5.png"
        title="/images/HMM2/HMM5.png" /></p>
</blockquote>
<p> </p>
<blockquote>
<h4 id="calculate-beta_1f-and-beta_1b">Calculate $\beta_1(F)$ and $\beta_1(B)$:</h4>
<p>$$
\begin{aligned}
\beta_1(F) &amp; = P(T_2, T_3|F_1) \\
&amp; = P(F_2 | F_1) \cdot P(T_2 | F_2) \cdot \color{red} \beta_2(F) \color{black} + P(B_2 | F_1) \cdot P(T_2 | B_2) \cdot \color{red} \beta_2(B) \color{black} \\
&amp; = 0.9 \times 0.5 \times 0.47 + 0.1 \times 0.2 \times 0.29 \\
&amp; = 0.2173
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\beta_1(B) &amp; = P(T_2, T_3|B_1) \\
&amp; = P(F_2 | B_1) \cdot P(T_2 | F_2) \cdot \color{red} \beta_2(F) \color{black} + P(B_2 | B_1) \cdot P(T_2 | B_2) \cdot \color{red} \beta_2(B) \color{black} \\
&amp; = 0.3 \times 0.5 \times 0.47 + 0.7 \times 0.2 \times 0.29 \\
&amp; = 0.1111
\end{aligned}
$$</p>
</blockquote>
<p> <br>
 </p>
<h4 id="forward--backward">Forward + backward</h4>
<p>Here is a summary of all $\alpha$ and $\beta$ we have calculated:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Quantity</th>
<th>$H_1$</th>
<th>$T_2$</th>
<th style="text-align:right">$T_3$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$\alpha(F)$</td>
<td>0.2</td>
<td>0.162</td>
<td style="text-align:right">0.08358</td>
</tr>
<tr>
<td style="text-align:left">$\alpha(B)$</td>
<td>0.48</td>
<td>0.0712</td>
<td style="text-align:right">0.013208</td>
</tr>
<tr>
<td style="text-align:left">$\beta(F)$</td>
<td>0.2173</td>
<td>0.47</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:left">$\beta(B)$</td>
<td>0.1111</td>
<td>0.29</td>
<td style="text-align:right">1</td>
</tr>
</tbody>
</table>
<p> </p>
<p>It turns out we can use $\alpha$ and $\beta$ to calculate another quantity $\gamma$, which represents the probability of having head/tail at that particular time point, given the observed data.</p>
<p> </p>
<p>For example, let&rsquo;s calculate $\gamma_2 (F)$</p>
<p>$$
\begin{aligned}
\gamma_2 (F) &amp; = P(F_2 |H_1, T_2, T_3 )\\
&amp;= \frac{\alpha_2(F) \cdot \beta_2(F) }{\alpha_2(F) \cdot \beta_2(F) + \alpha_2(B) \cdot \beta_2(B)} \\
&amp; = \frac{0.162 \times 0.47}{0.162 \times 0.47 + 0.0712 \times 0.29} \\
&amp; = 0.7867
\end{aligned}
$$</p>
<p>If you scroll up a little bit of this page, you should find this result is exactly the same as what we get using brutal force approach.</p>
<p> <br>
 </p>
<p>Let&rsquo;s try to calculate the probability of having a fair/biased coin for every single time point:</p>
<table>
<thead>
<tr>
<th style="text-align:left">P</th>
<th>$H_1$</th>
<th>$T_2$</th>
<th style="text-align:right">$T_3$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Fair</td>
<td>0.4490226</td>
<td>0.7866678</td>
<td style="text-align:right">0.8635368</td>
</tr>
<tr>
<td style="text-align:left">Biased</td>
<td>0.5509774</td>
<td>0.2133322</td>
<td style="text-align:right">0.1364632</td>
</tr>
</tbody>
</table>
<p> <br>
According to our final results table from forward-backward propagation, it looks like the hidden state is <strong>Biased &ndash; Fair &ndash; Fair</strong>, which is different from from our brutal force calculation. This is expected, since <strong>forward-backward algorithm tries to find the most likely state for any point of time, but it&rsquo;s not meant to find the most likely sequence of states</strong>.</p>
<p> <br>
 </p>
<p>Lastly, I want to prove the following equation:</p>
<p>$$P(F_2 |H_1, T_2, T_3 ) = \frac{\alpha_2(F) \cdot \beta_2(F) }{\alpha_2(F) \cdot \beta_2(F) + \alpha_2(B) \cdot \beta_2(B)} $$</p>
<p>We have
$$
\begin{aligned}
P(F_2 |H_1, T_2, T_3 ) &amp;= \frac{P(F_2 , H_1, T_2, T_3 )}{P(H_1, T_2, T_3)} \\
&amp; = \frac{P(F_2 , H_1, T_2, T_3 )}{P(F_2 , H_1, T_2, T_3 ) + P(B_2 , H_1, T_2, T_3 )} \\
&amp;= \frac{P(T_3 | H_1, T_2, F_2 ) \cdot P(H_1, T_2, F_2)}{P(T_3 | H_1, T_2, F_2 ) \cdot P(H_1, T_2, F_2) + P(T_3 | H_1, T_2, B_2 ) \cdot P(H_1, T_2, B_2)} \\
&amp; = \frac{P(T_3 | F_2 ) \cdot P(H_1, T_2, F_2)}{P(T_3 |  F_2 ) \cdot P(H_1, T_2, F_2) + P(T_3 | B_2 ) \cdot P(H_1, T_2, B_2)} \\
&amp;= \frac{\beta_2(F) \cdot \alpha_2(F)}{\beta_2(F) \cdot \alpha_2(F) + \beta_2(B) \cdot \alpha_2(B)} \\
Q.E.D
\end{aligned}
$$</p>
<p> <br>
 <br>
 <br>
 </p>
<p>We have covered quite a lot of materials in this blog post.  In the next blog post, I am planning to discuss viterbi algorithm, which is another greedy algorithm to infer the hidden states. Make sure to follow this series and leave comments if you have questions~</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2021-12-04</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://jasontan-code.github.io/posts/hmm2/" data-title="A random walk through HMM (2) - structure and inference" data-via="doubleTaoTan" data-hashtags="Probability"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://jasontan-code.github.io/posts/hmm2/" data-hashtag="Probability"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="Share on WhatsApp" data-sharer="whatsapp" data-url="https://jasontan-code.github.io/posts/hmm2/" data-title="A random walk through HMM (2) - structure and inference" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://jasontan-code.github.io/posts/hmm2/" data-title="A random walk through HMM (2) - structure and inference"><i data-svg-src="/lib/simple-icons/icons/line.min.svg"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://jasontan-code.github.io/posts/hmm2/" data-title="A random walk through HMM (2) - structure and inference"><i class="fab fa-weibo fa-fw"></i></a><a href="javascript:void(0);" title="Share on Myspace" data-sharer="myspace" data-url="https://jasontan-code.github.io/posts/hmm2/" data-title="A random walk through HMM (2) - structure and inference" data-description=""><i data-svg-src="/lib/simple-icons/icons/myspace.min.svg"></i></a><a href="javascript:void(0);" title="Share on Blogger" data-sharer="blogger" data-url="https://jasontan-code.github.io/posts/hmm2/" data-title="A random walk through HMM (2) - structure and inference" data-description=""><i class="fab fa-blogger fa-fw"></i></a><a href="javascript:void(0);" title="Share on Evernote" data-sharer="evernote" data-url="https://jasontan-code.github.io/posts/hmm2/" data-title="A random walk through HMM (2) - structure and inference"><i class="fab fa-evernote fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/probability/">Probability</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/hmm1/" class="prev" rel="prev" title="A random walk through HMM (1) - a background"><i class="fas fa-angle-left fa-fw"></i>A random walk through HMM (1) - a background</a></div>
</div>
<div id="comments"><div id="fb-root" class="comment"></div>
            <div
                class="fb-comments"
                data-href="https://jasontan-code.github.io/posts/hmm2/"
                data-width="100%"
                data-numposts="10"
            ></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://developers.facebook.com/docs/plugins/comments/"></a>Facebook</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.80.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/lightgallery/lightgallery.min.css"><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&amp;version=v5.0&amp;appId=2162149760591137&amp;autoLogAppEvents=1" defer></script><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lightgallery.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lg-thumbnail.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lg-zoom.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"lightGallery":{"actualSize":false,"exThumbImage":"data-thumbnail","hideBarsDelay":2000,"selector":".lightgallery","speed":400,"thumbContHeight":80,"thumbWidth":80,"thumbnail":true},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false}};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'G-C4MZRQHWVE', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=G-C4MZRQHWVE" async></script></body>
</html>
