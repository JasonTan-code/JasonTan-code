<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>All Posts - Taotao Tan</title>
        <link>https://jasontan-code.github.io/posts/</link>
        <description>All Posts | Taotao Tan</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 22 Nov 2021 15:18:26 -0600</lastBuildDate><atom:link href="https://jasontan-code.github.io/posts/" rel="self" type="application/rss+xml" /><item>
    <title>Maximum likelihood estimation</title>
    <link>https://jasontan-code.github.io/posts/mle/</link>
    <pubDate>Mon, 22 Nov 2021 15:18:26 -0600</pubDate>
    <author>Author</author>
    <guid>https://jasontan-code.github.io/posts/mle/</guid>
    <description><![CDATA[In this post, I will show you THE most important technique in inferential statistics: Maximum Likelihood Estimation (MLE).
 1. Some data to work with Before we get started, let&rsquo;s see what type of problem could be solved using MLE.
For example, I record the number of visitors of this website each hour from 8:00 am - 12:00 am (p.s. off course this is fake data, and I am probably too optimistic).]]></description>
</item><item>
    <title>Calculate PCA by hand</title>
    <link>https://jasontan-code.github.io/posts/calculate_pca/</link>
    <pubDate>Sun, 21 Nov 2021 15:18:26 -0600</pubDate>
    <author>Author</author>
    <guid>https://jasontan-code.github.io/posts/calculate_pca/</guid>
    <description><![CDATA[Here I am going to show you how to calculate PCA by hand!
But before we dive deep into PCA, there are two prerequisite concepts we need to understand:
 Variance/Covariance Find eigenvectors and eigenvalues  If you already understand those two concepts, you can skip this part.
 Prerequisite 1: Variance/Covariance  Variance Variance measures how far a set of numbers is spread out from their average value. The sample variance is defined as $$ s^2 = \frac{\sum(x_i - \bar x)^2}{n - 1} $$]]></description>
</item></channel>
</rss>
